Here‚Äôs a concise, execution-ready PRD for the seven upgrades‚Äîcentered on adding a two-LLM verifier while keeping your current architecture intact. No code included.

‚∏ª

PRD: ‚ÄúVerified California Law Chatbot v1.1‚Äù

0) Summary

We will harden truthfulness and verification without rearchitecting. The release introduces two-pass verification, stricter prompts, lightweight guardrails, and clearer source UX. Goal: >95% Verified Answer Rate (VAR) with minimal latency and cost impact.

Scope (this PRD):
	1.	System prompt hardening (no source, no claim; no new entities)
	2.	Two-pass verifier (second LLM call with rewrite-on-fail)
	3.	Regex guardrails (entity containment, citation existence, jurisdiction)
	4.	Quotes-only mode for high-risk categories
	5.	Source badges + disclaimer in UI
	6.	Light retrieval pruning (top-k=3, dedupe, lexical rerank)
	7.	Confidence gating (coverage threshold)

Non-Goals: New retrievers, vector DB, major UI redesign, multi-agent orchestration.

‚∏ª

1) Goals & Success Metrics
	‚Ä¢	Primary KPI: Verified Answer Rate (VAR) ‚â• 95% (answers requiring no verifier rewrite and containing only supported claims)
	‚Ä¢	Hallucination Rate: ‚â§ 2% (unsupported claims detected post-verification)
	‚Ä¢	Citation Precision: ‚â• 98% (every cite maps to a provided source)
	‚Ä¢	Latency budget: P50 ‚â§ 6.5s, P95 ‚â§ 10s (with verifier on)
	‚Ä¢	Cost delta: ‚â§ +45% vs. current (due to one extra LLM call), mitigated by selective verification + pruning
	‚Ä¢	User trust proxy: ‚ÄúWas this answer trustworthy?‚Äù thumbs-up ‚â• 80%

‚∏ª

2) Users & Use Cases
	‚Ä¢	Primary user: Public user asking CA law questions.
	‚Ä¢	Jobs-to-be-done: Get accurate, grounded answers with transparent citations; know when the system can‚Äôt verify.

‚∏ª

3) Architecture (Incremental)

Keep: UI ‚Üí Orchestrator/Middleware ‚Üí Retrieval (APIs) ‚Üí LLM.
Add: Verifier pass and pre-send guardrails.

User ‚Üí Generator LLM (A) ‚Üí (Answer + Claims) ‚Üí Verifier LLM (V) ‚Üí Guardrails ‚Üí UI
                    ‚Üë                         ‚Üë
                 SOURCES                  SOURCES

No new services or DBs. Optional in-memory/edge cache keyed by (question, selected passages).

‚∏ª

4) Feature Specs

4.1 System Prompt Hardening

Objective: Reduce creative drift; forbid unsupported content.

Requirements
	‚Ä¢	Add rules to the generator (A) and verifier (V) system prompts:
	‚Ä¢	No source, no claim.
	‚Ä¢	No new entities: Do not introduce case names, statute numbers, dates, dollar amounts, or thresholds not present in SOURCES.
	‚Ä¢	Cite-before-say: Select quotes first; every non-obvious claim requires an inline cite [id].
	‚Ä¢	Refusal policy: If support is insufficient/ambiguous: ‚ÄúI can‚Äôt verify this from the available sources.‚Äù
	‚Ä¢	CA-only by default (ignore non-CA unless asked).

Acceptance
	‚Ä¢	In a dry run of 50 queries with SOURCES removed, A refuses ‚â• 98% of the time.
	‚Ä¢	In a run with SOURCES present, A never emits entities not present in SOURCES (tested via regex containment).

‚∏ª

4.2 Two-Pass Verifier (Second LLM, Rewrite-on-Fail)

Objective: Turn ‚Äúagreeable‚Äù answers into evidence-bound answers.

Generator (A) Output Contract
	‚Ä¢	FINAL_ANSWER: natural language with inline [id] cites only from provided SOURCES.
	‚Ä¢	CLAIMS_JSON: array of atomic claims:
{ text, cites: [id], kind: 'statute'|'case'|'fact' }

Verifier (V) Input
	‚Ä¢	FINAL_ANSWER, CLAIMS_JSON, SOURCES[{id, title, url, excerpt}]

Verifier Tasks
	‚Ä¢	For each claim:
	‚Ä¢	Find 1‚Äì2 verbatim quotes in the cited source id(s).
	‚Ä¢	Mark supported/unsupported; collect evidence snippets.
	‚Ä¢	If any unsupported ‚Üí produce VERIFIED_REWRITE using only supported claims; maintain cites.
	‚Ä¢	If overall support thin ‚Üí return refusal line.
	‚Ä¢	Output VERIFICATION_REPORT summarizing per-claim support + quotes.

Gating
	‚Ä¢	If all claims supported ‚Üí return FINAL_ANSWER.
	‚Ä¢	Else ‚Üí return VERIFIED_REWRITE or refusal.

Acceptance
	‚Ä¢	On gold set (25 Qs), unsupported claims reduced by ‚â• 80% vs. baseline.
	‚Ä¢	No answer ships with a claim lacking evidence in VERIFICATION_REPORT.

Performance/Cost Controls
	‚Ä¢	V uses only cited passages, not all SOURCES.
	‚Ä¢	Selective verification: always on for high-risk categories; 50% sampling for low-risk (config).
	‚Ä¢	Cache by hash(question + passages + A output) TTL 24h.

‚∏ª

4.3 Regex Guardrails (Pre-Send)

Objective: Cheap, deterministic checks before display.

Checks
	1.	Entity containment: Any detected case pattern (\b[A-Z][a-z]+ v\. [A-Z]), statute (¬ß\s*\d+|Cal\. codes), numbers (\b\d{4}\b, \b\d+ (days|years)\b, \$\d+) must appear in at least one source excerpt. Else ‚Üí trigger V rewrite or refusal.
	2.	Citation existence: Every [id] in the answer must map to a known SOURCE.id.
	3.	Jurisdiction: Block reporters not in CA (e.g., F\.?3d, U\.S\.) unless the question requests federal.

Acceptance
	‚Ä¢	Synthetic tests with fake case names/thresholds are blocked 100% of time.
	‚Ä¢	No stray cite IDs allowed.

‚∏ª

4.4 Quotes-Only Mode (High-Risk Categories)

Objective: Max faithfulness when precision matters.

Triggers (heuristic keyword match in question or SOURCES):
penalty|punishable|years|days|deadline|statute of limitations|threshold|over \$|elements of|mens rea|burden

Behavior
	‚Ä¢	Step 1: Extract 2‚Äì4 exact quotes with [id].
	‚Ä¢	Step 2: Write a one-paragraph summary strictly from those quotes (no outside facts).
	‚Ä¢	Step 3: Add a single-line ‚ÄúPlain-English rule,‚Äù still cited.

Acceptance
	‚Ä¢	In triggered cases, every sentence traces to a quote; no paraphrase without a nearby cite.

‚∏ª

4.5 Source Badges + Disclaimer (UI)

Objective: Increase trust and compliance.

UI Changes
	‚Ä¢	Footnotes display badge labels:
üìñ Penal Code ¬ß187, ‚öñÔ∏è People v. Anderson (1972) (click ‚Üí source URL).
	‚Ä¢	Verified chip when V passes all claims; Partially verified when rewritten; Not verified only for explicit refusals (hidden by default).
	‚Ä¢	Fixed header disclaimer:
‚ÄúAI legal information, not legal advice. Verify sources; consult a licensed attorney for advice.‚Äù

Acceptance
	‚Ä¢	Badges present for 100% of cites; disclaimer always visible.
	‚Ä¢	A11y: badges accessible (aria-labels with source names).

‚∏ª

4.6 Light Retrieval Pruning

Objective: Reduce distractors that cause drift and cut token/cost.

Rules
	‚Ä¢	Top-k ‚â§ 3 passages post-pruning.
	‚Ä¢	De-dupe near-identical chunks by Jaccard similarity on tokens.
	‚Ä¢	Lexical rerank by simple overlap score with query (e.g., TF-IDF or token overlap ratio).
	‚Ä¢	Prefer statute-level or paragraph-level chunks (‚â§ ~800 tokens).

Acceptance
	‚Ä¢	Average tokens sent to A reduced by ‚â• 25% without harming answer coverage on gold set.
	‚Ä¢	VAR does not regress.

‚∏ª

4.7 Confidence Gating (Coverage Threshold)

Objective: Communicate verification quality; prevent marginal answers.

From V
	‚Ä¢	coverage = supported_claims / total_claims
	‚Ä¢	min_support = min #quotes per claim
	‚Ä¢	ambiguity = boolean (conflicting or generic sources)

Gates
	‚Ä¢	If coverage = 1.0 & min_support ‚â• 1 & ambiguity = false ‚Üí Verified.
	‚Ä¢	If 0.6 ‚â§ coverage < 1.0 ‚Üí Partially verified (ship with clear caveat line).
	‚Ä¢	If coverage < 0.6 or ambiguity = true ‚Üí Refusal line.

Acceptance
	‚Ä¢	No ‚ÄúVerified‚Äù label unless coverage == 1.0.
	‚Ä¢	User sees a one-sentence caveat in partially-verified answers.

‚∏ª

5) Telemetry & QA

Event Logging (PII-safe)
	‚Ä¢	retrieval.selected_ids, A.tokens, V.tokens, coverage, unsupported_claims, latency_ms, label (verified/partial/refusal), cost_cents.
	‚Ä¢	Hash IDs for queries/answers (content hashes), not raw text, unless user consents.

Dashboards
	‚Ä¢	Daily VAR, Hallucination rate, Citation precision.
	‚Ä¢	Latency P50/P95; token & cost per answer.

QA
	‚Ä¢	Maintain 25-item gold set with expected quotes.
	‚Ä¢	Pre-release: run full suite; must hit KPI thresholds.
	‚Ä¢	Weekly: random sample 50 live answers, human skim for correctness.

‚∏ª

6) Rollout Plan & Feature Flags

Flags
	‚Ä¢	verifier.enabled (default ON for high-risk; 50% sample for low-risk)
	‚Ä¢	quotes_only.enabled
	‚Ä¢	guardrails.enabled
	‚Ä¢	retrieval.pruning.enabled
	‚Ä¢	ui.badges.enabled, ui.disclaimer.enabled

Phases
	1.	Week 1: Prompt hardening + guardrails + UI badges/disclaimer (flags ON).
	2.	Week 2: Verifier on high-risk + confidence gating (monitor VAR & latency).
	3.	Week 3: Expand verifier sampling to 100% if latency within budget; tune pruning.

Rollback
	‚Ä¢	If P95 latency > 10s or error rate > 2%, auto-disable verifier and revert to quotes-only for high-risk.

‚∏ª

7) Risks & Mitigations
	‚Ä¢	Latency/cost creep: Use pruning, selective verification, passage-only context, caching by hash.
	‚Ä¢	Over-refusal: ‚ÄúPartially verified‚Äù path retains value while staying honest.
	‚Ä¢	Prompt injection attempts: Guardrails + no new entities + CA-only default.
	‚Ä¢	Source API outages: Show safe refusal + link to ‚Äútry again‚Äù with cached prior passages when available.

‚∏ª

8) Dependencies & Constraints
	‚Ä¢	Current LLM provider & API quotas.
	‚Ä¢	Stable access to CourtListener/OpenStates/LegiScan.
	‚Ä¢	No schema changes to retrieval outputs (must include {id, title, url, excerpt}).

‚∏ª

9) Acceptance Criteria (Release)
	‚Ä¢	VAR ‚â• 95% on gold set; citation precision ‚â• 98%; hallucination ‚â§ 2%.
	‚Ä¢	P50 latency ‚â§ 6.5s, P95 ‚â§ 10s (10-question mixed suite).
	‚Ä¢	UI displays source badges + disclaimer on all answers.
	‚Ä¢	No answer ships with unmatched cite IDs or entities absent from sources.

‚∏ª

10) Open Questions
	‚Ä¢	Thresholds for partial vs. refusal: keep 0.6 default or raise/lower after A/B?
	‚Ä¢	Expand high-risk triggers list based on live telemetry?
	‚Ä¢	Use a different model for V to avoid shared blind spots (future test)?

‚∏ª

If you want, I can turn this PRD into a Jira-ready breakdown (epics ‚Üí stories ‚Üí tasks) and draft the exact A/V prompt texts you‚Äôll paste into your config‚Äîstill no code.